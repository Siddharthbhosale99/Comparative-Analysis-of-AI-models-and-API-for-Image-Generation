{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fba9bfd7314349b0b3df1c0e0f14a950",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\siddh\\Downloads\\Case_Study_2\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "c:\\Users\\siddh\\Downloads\\Case_Study_2\\.venv\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from diffusers import DiffusionPipeline, UNet2DConditionModel, DDPMScheduler\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Step 1: Load your dataset (assumes the CSV has a column named 'prompt')\n",
    "class PromptDataset(Dataset):\n",
    "    def __init__(self, csv_file):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.prompts = self.data['Prompt']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.prompts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.prompts.iloc[idx]\n",
    "\n",
    "# Load your prompt dataset\n",
    "dataset = PromptDataset(\"Book12.csv\")\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "# Step 2: Load the diffusion model components\n",
    "pipeline = DiffusionPipeline.from_pretrained(\"prompthero/openjourney\")\n",
    "unet = pipeline.unet\n",
    "text_encoder = pipeline.text_encoder\n",
    "tokenizer = pipeline.tokenizer\n",
    "scheduler = pipeline.scheduler\n",
    "\n",
    "# Freeze all parameters of the text encoder and U-Net (optional, only if you want to fine-tune specific parts)\n",
    "for param in unet.parameters():\n",
    "    param.requires_grad = False  \n",
    "\n",
    "for param in text_encoder.parameters():\n",
    "    param.requires_grad = False  \n",
    "    \n",
    "# Step 3: Set up the optimizer\n",
    "optimizer = AdamW(list(unet.parameters()) + list(text_encoder.parameters()), lr=5e-5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]C:\\Users\\siddh\\AppData\\Local\\Temp\\ipykernel_21884\\1912282380.py:18: FutureWarning: Accessing config attribute `in_channels` directly via 'UNet2DConditionModel' object attribute is deprecated. Please access 'in_channels' over 'UNet2DConditionModel's config object instead, e.g. 'unet.config.in_channels'.\n",
      "  noise = torch.randn((input_ids.shape[0], unet.in_channels, 64, 64), requires_grad=True).to(device)\n",
      "100%|██████████| 3/3 [05:42<00:00, 114.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [06:05<00:00, 121.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [05:32<00:00, 110.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [06:06<00:00, 122.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5 completed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [05:30<00:00, 110.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5 completed.\n",
      "Fine-tuning completed and model saved.\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Training loop\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "unet.to(device)\n",
    "text_encoder.to(device)\n",
    "\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in tqdm(dataloader):\n",
    "        # Tokenize the prompts\n",
    "        inputs = tokenizer(list(batch), return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        input_ids = inputs.input_ids.to(device)\n",
    "\n",
    "        # Encode the text using the text encoder\n",
    "        encoder_hidden_states = text_encoder(input_ids)[0]\n",
    "\n",
    "        ## Generate random noise for diffusion (dummy noise, as you are not generating images)\n",
    "        noise = torch.randn((input_ids.shape[0], unet.in_channels, 64, 64), requires_grad=True).to(device)\n",
    "\n",
    "\n",
    "        # Get random timesteps for each sample in the batch\n",
    "        timesteps = torch.randint(0, scheduler.config.num_train_timesteps, (input_ids.shape[0],), device=device).long()\n",
    "\n",
    "        # Get model output by passing noise, timesteps, and encoder hidden states\n",
    "        model_output = unet(noise, timesteps, encoder_hidden_states).sample\n",
    "\n",
    "        # Define a dummy loss (MSE with random noise for simplicity)\n",
    "        loss = torch.nn.functional.mse_loss(model_output, noise)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} completed.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning completed and model saved.\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Save the fine-tuned model\n",
    "unet.save_pretrained(r\"C:\\Users\\siddh\\Downloads\\Case_Study_2\")\n",
    "text_encoder.save_pretrained(r\"C:\\Users\\siddh\\Downloads\\Case_Study_2\")\n",
    "tokenizer.save_pretrained(r\"C:\\Users\\siddh\\Downloads\\Case_Study_2\")\n",
    "\n",
    "print(\"Fine-tuning completed and model saved.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
